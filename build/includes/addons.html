
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>API Documentation</title>

    <style>
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight .gh {
  color: #999999;
}
.highlight .sr {
  color: #f6aa11;
}
.highlight .go {
  color: #888888;
}
.highlight .gp {
  color: #555555;
}
.highlight .gs {
}
.highlight .gu {
  color: #aaaaaa;
}
.highlight .nb {
  color: #f6aa11;
}
.highlight .cm {
  color: #75715e;
}
.highlight .cp {
  color: #75715e;
}
.highlight .c1 {
  color: #75715e;
}
.highlight .cs {
  color: #75715e;
}
.highlight .c, .highlight .cd {
  color: #75715e;
}
.highlight .err {
  color: #960050;
}
.highlight .gr {
  color: #960050;
}
.highlight .gt {
  color: #960050;
}
.highlight .gd {
  color: #49483e;
}
.highlight .gi {
  color: #49483e;
}
.highlight .ge {
  color: #49483e;
}
.highlight .kc {
  color: #66d9ef;
}
.highlight .kd {
  color: #66d9ef;
}
.highlight .kr {
  color: #66d9ef;
}
.highlight .no {
  color: #66d9ef;
}
.highlight .kt {
  color: #66d9ef;
}
.highlight .mf {
  color: #ae81ff;
}
.highlight .mh {
  color: #ae81ff;
}
.highlight .il {
  color: #ae81ff;
}
.highlight .mi {
  color: #ae81ff;
}
.highlight .mo {
  color: #ae81ff;
}
.highlight .m, .highlight .mb, .highlight .mx {
  color: #ae81ff;
}
.highlight .sc {
  color: #ae81ff;
}
.highlight .se {
  color: #ae81ff;
}
.highlight .ss {
  color: #ae81ff;
}
.highlight .sd {
  color: #e6db74;
}
.highlight .s2 {
  color: #e6db74;
}
.highlight .sb {
  color: #e6db74;
}
.highlight .sh {
  color: #e6db74;
}
.highlight .si {
  color: #e6db74;
}
.highlight .sx {
  color: #e6db74;
}
.highlight .s1 {
  color: #e6db74;
}
.highlight .s {
  color: #e6db74;
}
.highlight .na {
  color: #a6e22e;
}
.highlight .nc {
  color: #a6e22e;
}
.highlight .nd {
  color: #a6e22e;
}
.highlight .ne {
  color: #a6e22e;
}
.highlight .nf {
  color: #a6e22e;
}
.highlight .vc {
  color: #ffffff;
}
.highlight .nn {
  color: #ffffff;
}
.highlight .nl {
  color: #ffffff;
}
.highlight .ni {
  color: #ffffff;
}
.highlight .bp {
  color: #ffffff;
}
.highlight .vg {
  color: #ffffff;
}
.highlight .vi {
  color: #ffffff;
}
.highlight .nv {
  color: #ffffff;
}
.highlight .w {
  color: #ffffff;
}
.highlight {
  color: #ffffff;
}
.highlight .n, .highlight .py, .highlight .nx {
  color: #ffffff;
}
.highlight .ow {
  color: #f92672;
}
.highlight .nt {
  color: #f92672;
}
.highlight .k, .highlight .kv {
  color: #f92672;
}
.highlight .kn {
  color: #f92672;
}
.highlight .kp {
  color: #f92672;
}
.highlight .o {
  color: #f92672;
}
    </style>
    <link href="../stylesheets/screen.css" rel="stylesheet" media="screen" />
    <link href="../stylesheets/print.css" rel="stylesheet" media="print" />
      <script src="../javascripts/all_nosearch.js"></script>
  </head>

  <body class="includes includes_addons" data-languages="[]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="../images/navbar.png" alt="Navbar" />
      </span>
    </a>
    <div class="toc-wrapper">
      <div class="filtered-image">
      <img src="../images/logo.png" class="logo" alt="Logo" />
      </div>
      <ul id="toc" class="toc-list-h1">
          <li>
            <a href="#t-algorithmswithversion" class="toc-h1 toc-link" data-title="Algorithm V0.3.0">Algorithm V0.3.0</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#t-algorithmoverview" class="toc-h2 toc-link" data-title="Algorithm overview">Algorithm overview</a>
                  </li>
                  <li>
                    <a href="#t-algorithmdetails" class="toc-h2 toc-link" data-title="Algorithm details">Algorithm details</a>
                      <ul class="toc-list-h3">
                          <li>
                            <a href="#t-descriptionofparameters" class="toc-h3 toc-link" data-title="Description of parameters:">Description of parameters:</a>
                          </li>
                          <li>
                            <a href="#t-algo1ai" class="toc-h3 toc-link" data-title="1-a-i: Reconstructor Inspection">1-a-i: Reconstructor Inspection</a>
                          </li>
                          <li>
                            <a href="#t-algo1aii" class="toc-h3 toc-link" data-title="1-a-ii: Auto Classified Inspection">1-a-ii: Auto Classified Inspection</a>
                          </li>
                          <li>
                            <a href="#t-algo1bi" class="toc-h3 toc-link" data-title="1-b-i: Vector Inspection">1-b-i: Vector Inspection</a>
                          </li>
                          <li>
                            <a href="#t-algo1bii" class="toc-h3 toc-link" data-title="1-b-ii: Hunter Grove Inspection">1-b-ii: Hunter Grove Inspection</a>
                          </li>
                          <li>
                            <a href="#t-algo2i" class="toc-h3 toc-link" data-title="2-i: Mixed Grove Inspection">2-i: Mixed Grove Inspection</a>
                          </li>
                      </ul>
                  </li>
              </ul>
          </li>
      </ul>
    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        <h1 id='t-algorithmswithversion'>Algorithm V0.3.0</h1><h2 id='t-algorithmoverview'>Algorithm overview</h2>
<ul>
<li>Unsupervised learning (no annotation required)</li>
<li>Learn from ‘OK’ images only

<ul>
<li>Reconstructor Inspection</li>
<li>Auto Classified Inspection</li>
</ul></li>
<li>Learning from ‘OK’ images and small amount of ‘NG’ images

<ul>
<li>Vector Inspection</li>
<li>Hunter Grove Inspection</li>
</ul></li>
<li>Supervised learning (needs annotation)

<ul>
<li>Mixed Grove Inspection</li>
</ul></li>
</ul>
<h2 id='t-algorithmdetails'>Algorithm details</h2><h3 id='t-descriptionofparameters'>Description of parameters:</h3>
<ul>
<li>Model size (学習サイズ): Size of the model, larger models perform better but need longer to process</li>
<li>Range (型 : 範囲): Values used for this parameter need to be within this range of values</li>
<li>Variable name (変数名): Technical name of the parameter in the application</li>
<li>AutoTune (range): Parameters marked with “O” are optional. If not provided the system will automatically find the best value</li>
</ul>
<h3 id='t-algo1ai'>1-a-i: Reconstructor Inspection</h3>
<p>An unsupervised learning model that learns only from “OK” images (good samples, without defects). At the time of prediction, a reconstruction error is calculated and abnormality detection is performed based on the magnitude.
 <table class='hacarus-table'> <thead> <tr> <td><strong>Parameter</strong> </td> <td><strong>Variable name</strong> </td> <td><strong>Type: Range</strong> </td> <td><strong>Default</strong> </td> <td><strong>Impact on processing time</strong> </td> <td><strong>Impact on accuracy</strong> </td> <td><strong>AutoTune (range)</strong> </td> <td><strong>Remarks</strong> </td> </tr> </thead> <tbody> <tr> <td>Model size </td> <td>train_size </td> <td>int : [1, inf] </td> <td>10000 </td> <td>large -> large </td> <td>large -> large </td> <td>x </td> <td> </td> </tr> <tr> <td>Minimum inspection size </td> <td>min_inspection_size </td> <td>(int, int) : 0~height, 0~width </td> <td>(8, 8) </td> <td>large -> large </td> <td>- </td> <td>O (4, 8, 16) </td> <td>(4, 4) or (8, 8) or (16, 1)6 are empirically good. </td> </tr> <tr> <td>Minimum inspection size width </td> <td>min_inspection_step </td> <td>int : [1, inf] </td> <td>4 </td> <td>large -> small </td> <td>large -> small </td> <td>x </td> <td>A divisor of min_inspection_size is desirable. <p> 1 is the most computationally intensive, but the most efficient. </td> </tr> <tr> <td>Minimum detection size </td> <td>min_detected_area </td> <td>Float : [1, inf] </td> <td>None </td> <td>- </td> <td>- </td> <td>x </td> <td>Threshold of area of detected rectangle </td> </tr> <tr> <td>Magnification rate of abnormal part </td> <td>anomaly_magnification </td> <td>(int, int) : 0~height, 0~width </td> <td>(1, 1) </td> <td>- </td> <td>- </td> <td>o(1,3,5) </td> <td>If it is increased, small abnormal parts can be detected. However, larger values lead to over detection. </td> </tr> <tr> <td>Width from the perimeter to be ignored </td> <td>ignore_outer </td> <td>(int, int) : 0~height, 0~width </td> <td>(0,0) </td> <td>- </td> <td>- </td> <td>x </td> <td>The width of the ignored area from the perimeter <p> Use values up to the number of vertical and horizontal pixels in the image. </td> </tr> <tr> <td>Dimension of feature </td> <td>n_components </td> <td>int : [1, inf] </td> <td>10 </td> <td>large -> large </td> <td>large->more tolerant </td> <td>o(3, 5, 10, 20, 30) </td> <td>If the dimension of the feature amount is large, even complex objects can be reconstructed. </td> </tr> <tr> <td>Number of algorithm iterations </td> <td>max_iter </td> <td>Int : [1, inf] </td> <td>10 </td> <td>large -> large </td> <td>- </td> <td>x </td> <td>If the amount is too small, the performance will be degraded, but if it is somewhat large, the performance will not change much. Number of algorithm iterations </td> </tr> <tr> <td>Reconstruction error threshold </td> <td>reconstruct_thresh </td> <td>float : [0, inf] </td> <td>2.0 </td> <td>- </td> <td>large->more tolerant </td> <td>O () </td> <td>A threshold for reconstruction error, about 1.5 to 3 is empirically good </td> </tr> <tr> <td>Threshold value of detection area anomaly </td> <td>detected_area_thresh </td> <td>float : [0, inf] </td> <td>0.1 </td> <td>- </td> <td>- </td> <td>O ([0.1, 0.5]) </td> <td>Threshold value of detection area anomaly </td> </tr> <tr> <td>Algorithm type </td> <td>reconstruct_algorithm_name </td> <td>str : {‘nmf’’} </td> <td>‘nmf’ </td> <td>- </td> <td>- </td> <td>x </td> <td>Other types will be added later </td> </tr> </tbody> </table> </p>
<h3 id='t-algo1aii'>1-a-ii: Auto Classified Inspection</h3>
<p>An unsupervised learning model that learns only from “OK” images (non-defects). Deep learning is performed on a normal input image. At the time of inference, a reconstruction error is calculated for a new image, and abnormality detection is performed based on the magnitude.
 <table class='hacarus-table'> <tr> <td><strong>Parameter</strong> </td> <td><strong>Variable name</strong> </td> <td><strong>Type: Range</strong> </td> <td><strong>Default</strong> </td> <td><strong>Impact on processing time</strong> </td> <td><strong>Impact on accuracy</strong> </td> <td><strong>Auto Tune (range)</strong> </td> <td><strong>Remarks</strong> </td> </tr> <tr> <td>Model size </td> <td>train_size </td> <td>int : [1, inf] </td> <td>10000 </td> <td>large -> large </td> <td>large -> large </td> <td>x </td> <td> </td> </tr> <tr> <td>Minimum inspection size </td> <td>min_inspection_size </td> <td>(int, int) : 0~height, 0~width </td> <td>(8, 8) </td> <td>large -> large </td> <td>- </td> <td>O (4, 8, 16) </td> <td>(4, 4) or (8, 8) or (16, 1)6 are empirically good. </td> </tr> <tr> <td>Minimum inspection size width </td> <td>min_inspection_step </td> <td>int : [1, inf] </td> <td>4 </td> <td>large -> small </td> <td>large -> small </td> <td>x </td> <td>A divisor of min_inspection_size is desirable. <p> 1 is the most computationally intensive, but the most efficient. </td> </tr> <tr> <td>Minimum detection size </td> <td>min_detected_area </td> <td>Float : [1, inf] </td> <td>None </td> <td>- </td> <td>- </td> <td>x </td> <td>Threshold of area of detected rectangle </td> </tr> <tr> <td>Magnification rate of abnormal part </td> <td>anomaly_magnification </td> <td>(int, int) : 0~height, 0~width </td> <td>(1, 1) </td> <td>- </td> <td>- </td> <td>o(1,3,5) </td> <td>If it is increased, small abnormal parts can be detected. However, larger values lead to over detection. </td> </tr> <tr> <td>Width from the perimeter to be ignored </td> <td>ignore_outer </td> <td>(int, int) : 0~height, 0~width </td> <td>(0,0) </td> <td>- </td> <td>- </td> <td>x </td> <td>The width of the ignored area from the perimeter <p> Use values up to the number of vertical and horizontal pixels in the image. </td> </tr> <tr> <td>Layer structure </td> <td>encoding_dims </td> <td>Tuple[int] </td> <td>(128, 64, 32) </td> <td>large -> large </td> <td>large -> large </td> <td>x </td> <td>All values should be smaller than the product of min_detected_area. </td> </tr> <tr> <td>Epoch </td> <td>epochs </td> <td>int </td> <td>100 </td> <td>large -> large </td> <td>large -> large </td> <td>x </td> <td>1 or more integer value </td> </tr> <tr> <td>Batch size </td> <td>batch_size </td> <td>int </td> <td>256 </td> <td>large -> large </td> <td>- </td> <td>x </td> <td>A power of 2 less than train_size is preferred. </td> </tr> <tr> <td>Reconstruction error threshold </td> <td>reconstruct_thresh </td> <td>float : [0, inf] </td> <td>2.0 </td> <td>- </td> <td>large->more tolerant </td> <td>O () </td> <td>A threshold for reconstruction error, about 1.5 to 3 is empirically good </td> </tr> <tr> <td>Threshold value of detection area anomaly </td> <td>detected_area_thresh </td> <td>float : [0, inf] </td> <td>0.1 </td> <td>- </td> <td>- </td> <td>O ([0.1, 0.5]) </td> <td>Threshold value of detection area anomaly </td> </tr> </table> </p>
<h3 id='t-algo1bi'>1-b-i: Vector Inspection</h3>
<p>Unsupervised learning model to learn from “OK” images (non-defects) and a small amount of “NG” images (with defects).
 <table class='hacarus-table'> <tr> <td><strong>Parameter</strong> </td> <td><strong>Variable name</strong> </td> <td><strong>Type: Range</strong> </td> <td><strong>Default</strong> </td> <td><strong>Impact on processing time</strong> </td> <td><strong>Impact on accuracy</strong> </td> <td><strong>Auto Tune (range)</strong> </td> <td><strong>Remarks</strong> </td> </tr> <tr> <td>Model size </td> <td>train_size </td> <td>int : [1, inf] </td> <td>10000 </td> <td>large -> large </td> <td>large -> large </td> <td>x </td> <td> </td> </tr> <tr> <td>Minimum inspection size </td> <td>min_inspection_size </td> <td>(int, int) : 0~height, 0~width </td> <td>(8, 8) </td> <td>large -> large </td> <td>- </td> <td>O (4, 8, 16) </td> <td>(4, 4) or (8, 8) or (16, 1)6 are empirically good. </td> </tr> <tr> <td>Minimum inspection size width </td> <td>min_inspection_step </td> <td>int : [1, inf] </td> <td>4 </td> <td>large -> small </td> <td>large -> small </td> <td>x </td> <td>A divisor of min_inspection_size is desirable. <p> 1 is the most computationally intensive, but the most efficient. </td> </tr> <tr> <td>Minimum detection size </td> <td>min_detected_area </td> <td>Float : [1, inf] </td> <td>None </td> <td>- </td> <td>- </td> <td>x </td> <td>Threshold of area of detected rectangle </td> </tr> <tr> <td>Magnification rate of abnormal part </td> <td>anomaly_magnification </td> <td>(int, int) : 0~height, 0~width </td> <td>(1, 1) </td> <td>- </td> <td>- </td> <td>o(1,3,5) </td> <td>If it is increased, small abnormal parts can be detected. However, larger values lead to over detection. </td> </tr> <tr> <td>Width from the perimeter to be ignored </td> <td>ignore_outer </td> <td>(int, int) : 0~height, 0~width </td> <td>(0,0) </td> <td>- </td> <td>- </td> <td>x </td> <td>The width of the ignored area from the perimeter <p> Use values up to the number of vertical and horizontal pixels in the image. </td> </tr> <tr> <td>Percentage of anomalies in the data set </td> <td>contamination </td> <td>Float : [0, 0.5] </td> <td>0.01 </td> <td>- </td> <td>Large -> less tolerant </td> <td>o([0.0001, 0.5]) </td> <td>A value close to the percentage of anomalies in the dataset is desirable </td> </tr> </table> </p>
<h3 id='t-algo1bii'>1-b-ii: Hunter Grove Inspection</h3>
<p>Unsupervised learning model to learn from “OK” images (non-defects) and a small amount of “NG” images (with defects).
 <table class='hacarus-table'> <tr> <td><strong>Parameter</strong> </td> <td><strong>Variable name</strong> </td> <td><strong>Type: Range</strong> </td> <td><strong>Default</strong> </td> <td><strong>Impact on processing time</strong> </td> <td><strong>Impact on accuracy</strong> </td> <td><strong>Auto Tune (range)</strong> </td> <td><strong>Remarks</strong> </td> </tr> <tr> <td>Model size </td> <td>train_size </td> <td>int : [1, inf] </td> <td>10000 </td> <td>large -> large </td> <td>large -> large </td> <td>x </td> <td> </td> </tr> <tr> <td>Minimum inspection size </td> <td>min_inspection_size </td> <td>(int, int) : 0~height, 0~width </td> <td>(8, 8) </td> <td>large -> large </td> <td>- </td> <td>O (4, 8, 16) </td> <td>(4, 4) or (8, 8) or (16, 1)6 are empirically good. </td> </tr> <tr> <td>Minimum inspection size width </td> <td>min_inspection_step </td> <td>int : [1, inf] </td> <td>4 </td> <td>large -> small </td> <td>large -> small </td> <td>x </td> <td>A divisor of min_inspection_size is desirable. <p> 1 is the most computationally intensive, but the most efficient. </td> </tr> <tr> <td>Minimum detection size </td> <td>min_detected_area </td> <td>Float : [1, inf] </td> <td>None </td> <td>- </td> <td>- </td> <td>x </td> <td>Threshold of area of detected rectangle </td> </tr> <tr> <td>Magnification rate of abnormal part </td> <td>anomaly_magnification </td> <td>(int, int) : 0~height, 0~width </td> <td>(1, 1) </td> <td>- </td> <td>- </td> <td>o(1,3,5) </td> <td>If it is increased, small abnormal parts can be detected. However, larger values lead to over detection. </td> </tr> <tr> <td>Width from the perimeter to be ignored </td> <td>ignore_outer </td> <td>(int, int) : 0~height, 0~width </td> <td>(0,0) </td> <td>- </td> <td>- </td> <td>x </td> <td>The width of the ignored area from the perimeter <p> Use values up to the number of vertical and horizontal pixels in the image. </td> </tr> <tr> <td>Dimension of feature </td> <td>n_estimators </td> <td>Int : [1, inf] </td> <td>100 </td> <td>large -> large </td> <td>large -> large </td> <td>O (10, 50, 100, 200) </td> <td> </td> </tr> <tr> <td>Percentage of anomalies in the data set </td> <td>contamination </td> <td>Float : [0, 0.5] </td> <td>0.01 </td> <td>- </td> <td>Large -> less tolerant </td> <td>o([0.0001, 0.5]) </td> <td>A value close to the percentage of anomalies in the dataset is desirable </td> </tr> </table> </p>
<h3 id='t-algo2i'>2-i: Mixed Grove Inspection</h3>
<p>A supervised learning model (requires annotation) that learns from both “OK” images (non-defects) and “NG” images (with detefects).
 <table class='hacarus-table'> <thead> <tr> <td><strong>Parameter</strong> </td> <td><strong>Variable name</strong> </td> <td><strong>Type: Range</strong> </td> <td><strong>Default</strong> </td> <td><strong>Impact on processing time</strong> </td> <td><strong>Impact on accuracy</strong> </td> <td><strong>Auto Tune (range)</strong> </td> <td><strong>Remarks</strong> </td> </tr> </thead> <tbody> <tr> <td>Model size </td> <td>train_size </td> <td>int : [1, inf] </td> <td>10000 </td> <td>large -> large </td> <td>large -> large </td> <td>x </td> <td> </td> </tr> <tr> <td>Minimum inspection size </td> <td>min_inspection_size </td> <td>(int, int) : 0~height, 0~width </td> <td>(8, 8) </td> <td>large -> large </td> <td>- </td> <td>O (4, 8, 16) </td> <td>(4, 4) or (8, 8) or (16, 1)6 are empirically good. </td> </tr> <tr> <td>Minimum inspection size width </td> <td>min_inspection_step </td> <td>int : [1, inf] </td> <td>4 </td> <td>large -> small </td> <td>large -> small </td> <td>x </td> <td>A divisor of min_inspection_size is desirable. <p> 1 is the most computationally intensive, but the most efficient. </td> </tr> <tr> <td>Minimum detection size </td> <td>min_detected_area </td> <td>Float : [1, inf] </td> <td>None </td> <td>- </td> <td>- </td> <td>x </td> <td>Threshold of area of detected rectangle </td> </tr> <tr> <td>Magnification rate of abnormal part </td> <td>anomaly_magnification </td> <td>(int, int) : 0~height, 0~width </td> <td>(1, 1) </td> <td>- </td> <td>- </td> <td>o(1,3,5) </td> <td>If it is increased, small abnormal parts can be detected. However, larger values lead to over detection. </td> </tr> <tr> <td>Width from the perimeter to be ignored </td> <td>ignore_outer </td> <td>(int, int) : 0~height, 0~width </td> <td>(0,0) </td> <td>- </td> <td>- </td> <td>x </td> <td>The width of the ignored area from the perimeter <p> Use values up to the number of vertical and horizontal pixels in the image. </td> </tr> <tr> <td>Dimension of feature </td> <td>n_estimators </td> <td>Int : [1, inf] </td> <td>100 </td> <td>large -> large </td> <td>large -> large </td> <td>O (10, 50, 100, 200) </td> <td> </td> </tr> <tr> <td>Threshold value of detection area anomaly </td> <td>detected_area_thresh </td> <td>float : [0, inf] </td> <td>0.1 </td> <td>- </td> <td>- </td> <td>O ([0.1, 0.5]) </td> <td>Threshold value of detection area anomaly </td> </tr> </tbody> </table> </p>

      </div>
      <div class="dark-box">
      </div>
    </div>
  </body>
</html>
